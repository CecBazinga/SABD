Script e report per la creazione dell'ambiente di lavoro (Ubuntu 20.04)

 - Installare IntelliJ IDEA con JDK 1.8

 $ sudo apt-get update
 $ sudo apt install vim
 $ sudo apt install git
 
 - Installare Containerd, Docker CE CLI e Docker CE (Engine) da: https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/
   (Per installarli è necessario fare $ sudo dpkg -i path/to/package nell'ordine specificato sopra.)
 
 - [OPZIONALE] $ sudo docker run hello-world per testare docker
 
 


 ****[SETUP DI HADOOP]****
 
 	- Scaricare gli script di Fabiana Rossi (Presenti nel sito della Cardellini alla lezione di Hadoop)
 	
 	- E' stato fatto il cambio versione da Hadoop 3.1.4 a 3.2.2
 	
 	- Andare nella cartella scripts ed eseguire $ sudo ./start.sh il comando crea la build di docker (build-docker.sh), crea un network ed esegue su due terminali diversi il cluster ed il client (start-dockers.sh e strart-client-sh)
 	
 	- [OPZIONALE] Fare il check del corretto funzionamento con $ hdfs dfsadmin -report, oppure collegandoci a http://localhost:9870/dfshealth.html
 	
 	- [OPZIONALE] Sul master node di Hadoop, aprire /etc/hadoop/conf/capacity-scheduler.xml e modificare il campo "yarn.scheduler.capacity.maximum-am-resource" alzandolo a 0.5 circa (Migliori performance, ma niente di critico per il run di una singola applicazione)
 	
 	- [SUL CLIENT] Andare nella cartella /usr/local ed eseguire il comando:
 		$ spark-submit --class main.java.MyFuckingClass --master yarn --deploy-mode client sparkapp.jar 
 	   Questo comando farà partire il job di Spark e lo eseguirà sui nodi del cluster Hadoop. 
 	   
 	   
 	- Una volta finita l'esecuzione, è possibile controllare lo stato del sistema collegandosi a:
 		+ http://localhost:8088 (Hadoop e YARN)
 		+ http://localhost:9870 (HDFS)  
 		+ http://localhost:4040 (Spark)  
 	
 	- Uscire da entrambi i terminali con CTRL + D	
 	
 	- Eseguire $ sudo ./stop-docker.sh per terminare i container e pulire l'ambiente
 	
 	
 	
 	
 	
 	[APPUNTI DA SISTEMARE]
 	
 	spark-submit --class it.sabd.MyFuckingClass --master yarn --deploy-mode client sparkapp.jar 
 	
 	in Hadoop /etc/hadoop/conf/capacity-scheduler.xml il campo yarn.scheduler.capacity.maximum-am-resource alzaro a 0.5
 	
 	vedi i siti su internet
  
  fare cluster mode o client mode per spark
  
  Siamo sicuri che l'ordinamento va fatto cos? non è troppo semplice?
  
  nifi levare colonne superflue subito
  
 
 
 
 
 
 


[NOTE PER LA RELAZIONE]

- Abbiamo usato build iun locale invece che su amazon per familiarizzare con l'instansiazione della roba manualmente (AWS fa tutto in automatico)

- Usiamo Parquet perchè è column-based ed è ottimale per analytic reads

- L'applicazione client dovrà girare all'interno dell'ambiente docker per permettere l'accesso alla sottorete (con tutte le porte e gli IP del cluster hadoop)

- I Namenode di HFDS e Workernode di YARN si trovano sulla stessa macchina come il Resource Manager (YARN) e Masternode (HDFS)?

- Aggiunti lo yarn-site.xml per gestire il funzionamento di YARN
